\documentclass[a4paper, 11pt]{exam}
\usepackage{titling}
\usepackage{algorithm,algorithmic,amsmath}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    }%
}

\usepackage{url}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{caption}
\usepackage{amsmath,amsthm,enumitem,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\renewcommand{\labelenumii}{\roman{enumii}}

\title{Homework Assignment 3}
\subtitle{CS/ECE 6810: Computer Architecture \\
October 31,2018
\\
Name: Jake Pitkin

UID: u0891770}

\author{ \\
\textbf{OoO and Cache Optimization}}
\date{Due Date: November 11, 2018.\\
120 points}

\begin{document}
\maketitle

\begin{enumerate}


\item \textbf{OoO Processor and Cache Parameters.} Please specify each of the following statement is True or False and explain why. 

\begin{enumerate}
	\item Instructions read all source operands from the register file in the Tomasuloâ€™s algorithm.\textbf{(2 points)}
	
	\fbox{\parbox{\linewidth}{\textbf{False.} A source operand could also be read from a reservation station as Tomasulo's algorithm uses these for register renaming.}}
	
	\item In a processor with hardware speculation, a store can always be committed as soon
	as its effective address and source value are available in the load-store queue.\textbf{(2 points)}
	
	\fbox{\parbox{\linewidth}{\textbf{True.} Effective address is required for dependence check and we check availability of operands every cycle to determine if we can execute the store operation.}}
	
	\item If cache block size remains same, but the number of cache blocks doubles, the compulsory misses remain the same.\textbf{(2 points)}
	
	\fbox{\parbox{\linewidth}{\textbf{False.} A compulsory miss is the first access to a block. With twice as many cache blocks, we increase the number of possible compulsory misses. Increasing the cache block size instead could decrease the number of compulsory misses.}}
	
	\item  If the cache capacity is fixed but the block size is increased, the miss rate will not change.\textbf{(2 points)}

	\fbox{\parbox{\linewidth}{\textbf{False.} The number of compulsory misses will decrease as the block size is increasing. Additionally, changing the cache design will most likely change the conflict miss rate (depending on the access pattern of a given program) compared to the miss rate of the original design.}}
	
	\item Write-through caches use write allocate mechanism to prevent writing dirty blocks to lower memory levels.\textbf{(2 points)}
	
	\fbox{\parbox{\linewidth}{\textbf{False.} A write-through cache typically uses no-write allocate. No real benefit to cache what was written as a future matching write (when using write-through) will write through to memory regardless.}}
\end{enumerate}



\item \textbf {Memory Hierarchy.}
Consider a processor with the following memory
organization: L1 Cache, L2 Cache, L3 Cache and main memory. Each cache stores both tags
and data. The data and tag arrays are going to be accessed with the same index value. Assume
that the processor does serial tag/data look-up (first tag lookup and then data access) for L2
and L3 caches and parallel tag/data look-up for L1 cache. The table given below provides the
time take to access the tag and data arrays in CPU cycles. 
Find the number of cycles required to complete 2000 load instructions accessing this
hierarchy.\textbf{(15 points)}

\begin{center}
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		\textbf{Level}& \textbf{Tag Access}& \textbf{Data Access} & \textbf{Hit Rate}\\ 
		\hline
		L1 (32 KB) & 1 & -  & 40\%\ \\ 
		L2 (1 MB) & 4 & 12  & 50\% \\ 
		L3 (8 MB) & 25 & 90  & 80\%\\ 
		\hline
		Main Memory & - & 500  & - \\ 
		\hline
	\end{tabular}
\end{center}

\begin{align*}
L1 \ hit \ cycles &= instructions * access \ time * L1_{hit} \\ 
            &= 2,000 * 1 * 0.4 \\
            &= 800 \\
L2 \ hit \ cycles &= instructions * access \ time * L1_{miss} * L2_{hit} \\
            &= 2,000 * (1 + 4 + 12) * 0.6 * 0.5\\
            &= 8,160 \\
L3 \ hit cycles &= instructions * access \ time * L1_{miss} * L2_{miss} * L3_{hit}\\
            &= 2,000 * (1 + 4 + 25 + 90) * 0.6 * 0.5 * 0.8\\
            &= 57,600\\
Main \ Memory \ Cycles &= instructions * access \ time * L1_{miss} * L2_{miss} * L3_{miss} \\
                       &= 2,000 * (1 + 4 + 25 + 500) * 0.6 * 0.5 * 0.2\\
                       &= 63,600\\
Total \ cycles &= L1_{cycles} + L2_{cycles} + L3_{cycles} + Memory_{cycles}\\
               &= 800 + 8,160 + 57,600 + 63,600 \\
               &= 130,160 \ cycles                      
\end{align*}

\fbox{\parbox{\linewidth}{\textbf{Answer:} 130,160 cycles to complete 2000 load instructions.}}
\newpage
\item \textbf{Cache Hit Rate.}
Consider a 256K main memory system with a direct mapped cache that stores up to 4 blocks. Each cache block comprises 4 words. The replacement policy is MRU and at the beginning cache is empty. Compute the hit rate for the following stream of addresses generated by the processor.(All memory addresses are in decimal format and each address refer to a word).\textbf{(10 points)}

\texttt{170 , 257, 168, 246, 176, 175, 176, 177, 175, 176, 177, 175, 176, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 168, 165, 164}

\textbf{Assumptions:} Memory is zero-indexed. The replacement policy is unused as this is a direct mapped cache. We fetch a block of 4 contiguous words at a time when accessing memory and caching.

To determine the location where a block of memory will be stored in the cache we can first calculate its block address and then mod that by the number of blocks in the cache. This is similar to modulo hashing done in the slides but we first break memory up into 4 word blocks.

\begin{align*}
Cache \ Location &= Block \ Address \ \% \ Cache \ Size \\
Cache \ Location &= \floor*{address / 4} \ \% \ 4
\end{align*}

\textbf{1. Address: 170, Block: 168-171, Cache Location: 2, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  &  & & \\ 
		\hline
		\textbf{Block 1} &  &   &  &\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} &  &   &  &\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{2. Address: 257, Block: 256-259, Cache Location: 0, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  256 &  257 & 258 & 259\\ 
		\hline
		\textbf{Block 1} &  &   &  &\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} &  &   &  &\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{3. Address: 168, Hit/Miss: Hit.}

\textbf{4. Address: 246, Block: 244-247, Cache Location: 1, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  256 &  257 & 258 & 259\\ 
		\hline
		\textbf{Block 1} &  244 & 245 & 246 & 247\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} &  &   &  &\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{5. Address: 176, Block: 176-179, Cache Location: 0, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  176 &  177 & 178 & 179\\ 
		\hline
		\textbf{Block 1} &  244 & 245 & 246 & 247\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} &  &   &  &\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{6. Address: 175, Block: 172-175, Cache Location: 3, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  176 &  177 & 178 & 179\\ 
		\hline
		\textbf{Block 1} &  244 & 245 & 246 & 247\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} & 172 & 173 & 174 & 175\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{7. Address: 176, Hit/Miss: Hit.}

\textbf{8. Address: 177, Hit/Miss: Hit.}

\textbf{9. Address: 175, Hit/Miss: Hit.}

\textbf{10. Address: 176, Hit/Miss: Hit.}

\textbf{11. Address: 177, Hit/Miss: Hit.}

\textbf{12. Address: 175, Hit/Miss: Hit.}

\textbf{13. Address: 176, Hit/Miss: Hit.}

\textbf{14. Address: 177, Hit/Miss: Hit.}

\textbf{15. Address: 176, Hit/Miss: Hit.}

\textbf{16. Address: 175, Hit/Miss: Hit.}

\textbf{17. Address: 174, Hit/Miss: Hit.}

\textbf{18. Address: 173, Hit/Miss: Hit.}

\textbf{19. Address: 172, Hit/Miss: Hit.}

\textbf{20. Address: 171, Hit/Miss: Hit.}

\textbf{21. Address: 170, Hit/Miss: Hit.}

\textbf{22. Address: 169, Hit/Miss: Hit.}

\textbf{23. Address: 168, Hit/Miss: Hit.}

\textbf{24. Address: 167, Hit/Miss: Hit.}

\textbf{25. Address: 168, Hit/Miss: Hit.}

\textbf{26. Address: 165, Block: 164-167, Cache Location: 1, Hit/Miss: Miss.}

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		&\textbf{Word 1}& \textbf{Word 2}& \textbf{Word 3} & \textbf{Word 4}\\ 
		\hline
		\textbf{Block 0} &  176 &  177 & 178 & 179\\ 
		\hline
		\textbf{Block 1} &  164 & 165 & 166 & 167\\ 
		\hline
		\textbf{Block 2} & 168 & 169  & 170 & 171\\ 
		\hline
		\textbf{Block 3} & 172 & 173 & 174 & 175\\ 
		\hline
	\end{tabular}
\end{center}

\textbf{27. Address: 164, Hit/Miss: Hit.}

\fbox{\parbox{\linewidth}{Lookups: $27$ \\ Hits: $21$ \\Misses: $6$ \\ Hit rate: $21/27$ or $77.78\%$}}


\item \textbf{Cache Performance.}
Consider the following  pseudo code; suppose that \textbf{X} and \textbf{Y} are allocated as contiguous integer arrays in memory and are aligned to the 4KB boundaries.
Assume \textbf{k} and \textbf{l} are allocated in the register file with no need for memory accesses.
We execute the code on a machine where the integer type (\textbf{int}) is 4 bytes wide.

	\begin{algorithm}
		\textbf{\#define M 1024} \\
		\textbf{int X[M*M];} \\
		\textbf{int Y[M*M];} \\
		\textbf{int k, l;}
		\begin{algorithmic}	
			\FOR{$(k = 0; k < M; k++)$}
			\FOR{$(l = 0; l < M; l++)$}
			\STATE \texttt{Y[l*M+k] = X[k*M+l];}
			\ENDFOR
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}

\begin{enumerate}
\item Consider a 4KB 2-way set-associative cache architecture while cache block size is 32-byte (8-word) and the replacement policy is LRU. Compute the hit rate of data accesses generated by \texttt{load}s and \texttt{store}s to \textbf{X} and \textbf{Y}?\textbf{(15 points)}




\item Consider a 4KB fully associative cache architecture with 32-byte blocks. The replacement policy is LRU. Rewrite the code to remove  all of the non-compulsory misses. (You need to ensure the new code generate the exact same output in the main memory.You are allowed to add a nested for loop to the code if necessary.) Please provide explanation on how the new code can remove those misses.\textbf{(15 points)}


\end{enumerate}



\item \textbf {Cache Addressing.} Consider a processor using 3 cache levels. Level-1 cache is a
32KB direct mapped cache with 16B blocks used for both instructions and data. Level-2 is a
256KB, 2-way set associative cache with 64B cache lines. Level-3 is a
1MB, 4-way set associative cache with 64B cache lines. Assume that the processor can
address up to 16GB of main memory.  Compute the size of the tag arrays in KB for each level.\textbf{(15 points)}


\textbf{Level-1 Cache}

First we need to calculate the tag bits to know the size of the tag arrays for level. This can be done by considering the address bits and how those are distributed to the byte offset, index, and tag bits:

\begin{align*}
\text{address bits} &= \text{34 bits} \  (16GB = 2^{34} B) \\
\text{byte offset bits} &= \text{4 bits} \ (16B = 2^4) \\
\text{index bits} &= \text{32KB/16B} = \text{11 bits} \ (2^{11})\\
\text{tag bits} &= 34 - 4 - 11 = 19 \ \text{bits}         
\end{align*}

For each block there will be a tag so we multiply the number of tag bits by the number of blocks:

\begin{align*}
\text{tag array size for L1} &= 19 \ \text{bits} * 2^{11} = 39, 912 \text{bits} = 4.989 \ \text{KB} 	
\end{align*}

\textbf{Level-2 Cache}

\textbf{Assumption:} "Cache line" means cache block and not cache row (as per Canvas discussion). Therefore a cache row is 2 * 64B = 128B wide.\\ \\
Level two will have the same address bits but we will recalculate the distribution of the other bits:

\begin{align*}
\text{address bits} &= \text{34 bits} \  (16GB = 2^{34} B) \\
\text{byte offset bits} &= \text{6 bits} \ (64B = 2^6) \\
\text{index bits} &= \text{256KB/(2 * 64B)} = \text{11 bits} \ (2^{11})\\
\text{tag bits} &= 34 - 6 - 11 =  17 \ \text{bits}         
\end{align*}

For each block there will be a tag so we multiply the number of tag bits by the number of blocks:

\begin{align*}
\text{tag array size for L2} &= 17 \ \text{bits} * 2 * 2^{11} = 69,632 \text{bits} = 8.704 \ \text{KB} 	
\end{align*}

\textbf{Level-3 Cache}

\textbf{Assumption:} "Cache line" means cache block and not cache row (as per Canvas discussion). Therefore a cache row is 4 * 64B = 256B wide.\\ \\

Level three will have the same address bits but we will recalculate the distribution of the other bits:

\begin{align*}
\text{address bits} &= \text{34 bits} \  (16GB = 2^{34} B) \\
\text{byte offset bits} &= \text{6 bits} \ (64B = 2^6) \\
\text{index bits} &= \text{1MB/(4 * 64B)} = \text{12 bits} \ (2^{12})\\
\text{tag bits} &= 34 - 6 - 12 =  16 \ \text{bits}         
\end{align*}

For each block there will be a tag so we multiply the number of tag bits by the number of blocks:

\begin{align*}
\text{tag array size for L3} &= 16 \ \text{bits} * 4 * 2^{12} = 262,144 \text{bits} = 32.768 \ \text{KB} 	
\end{align*}

\fbox{\parbox{\linewidth}{L1: $4.989KB$ \\ 
L2: $8.704KB$ \\
L3: $32.768KB$ \\ 
Total: $46.461KB$}}

\item \textbf{Cache and Memory Model using CACTI }

CACTI (\url{http://www.cs.utah.edu/~rajeev/cacti7/}) is an integrated cache and memory model for access time, cycle time, area, leakage, and dynamic power. You are asked to use CACTI 7 for investigating the impact of small and simple caches using a 22nm CMOS technology node. Consider a processor using 2 cache levels. Level-1 cache is a 32 KB direct mapped cache with 16B blocks used for both instructions and data. Level-2 is a 1MB, 4-way set associative cache with 64B cache lines. Assume that the processor can address up to 2GB of main memory. Assume that both of the Level-1 and Level-2 caches are single bank.
\begin{enumerate}
	\item Compare the access time, energy, leakage power, and area of the caches mentioned above.\textbf{(10 points)} 
    \item Investigate the impact of varying associativity for 1, 2, and 8 on the access time, energy, leakage, and area of the Level-2 cache.\textbf{(10 points)}

\end{enumerate}

\end{enumerate}


\end{document}